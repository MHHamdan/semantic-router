{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1AJs8NHh6tF"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/semantic-router/blob/main/docs/09-route-filter.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/semantic-router/blob/main/docs/00-introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tly249bZh6tG"
   },
   "source": [
    "# Semantic Router Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obVbdRpkh6tH"
   },
   "source": [
    "The Semantic Router library can be used as a super fast route making layer on top of LLMs. That means rather than waiting on a slow agent to decide what to do, we can use the magic of semantic vector space to make routes. Cutting route making time down from seconds to milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone[grpc]\n",
      "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from pinecone[grpc]) (2024.12.14)\n",
      "Collecting googleapis-common-protos>=1.53.0 (from pinecone[grpc])\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio>=1.59.0 (from pinecone[grpc])\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting lz4>=3.1.3 (from pinecone[grpc])\n",
      "  Downloading lz4-4.4.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone[grpc])\n",
      "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone[grpc])\n",
      "  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting protobuf<5.0,>=4.25 (from pinecone[grpc])\n",
      "  Downloading protobuf-4.25.6-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone[grpc])\n",
      "  Using cached protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from pinecone[grpc]) (2.9.0.post0)\n",
      "Collecting tqdm>=4.64.1 (from pinecone[grpc])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from pinecone[grpc]) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from pinecone[grpc]) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\awesome\\anaconda3\\envs\\py12\\lib\\site-packages (from tqdm>=4.64.1->pinecone[grpc]) (0.4.6)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.0/4.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.3/4.3 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.4/4.3 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading lz4-4.4.3-cp312-cp312-win_amd64.whl (99 kB)\n",
      "Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
      "Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading protobuf-4.25.6-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
      "Installing collected packages: tqdm, protobuf, pinecone-plugin-interface, lz4, grpcio, pinecone-plugin-inference, googleapis-common-protos, protoc-gen-openapiv2, pinecone\n",
      "Successfully installed googleapis-common-protos-1.66.0 grpcio-1.70.0 lz4-4.4.3 pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 protobuf-4.25.6 protoc-gen-openapiv2-0.0.1 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pinecone[grpc]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index stats:\n",
      " {'dimension': 2,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'example-namespace': {'vector_count': 3}},\n",
      " 'total_vector_count': 3}\n",
      "\n",
      "Query response:\n",
      " {'matches': [{'id': 'vec2',\n",
      "              'metadata': {'genre': 'documentary'},\n",
      "              'score': 1.0,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': [3.0, -2.0]}],\n",
      " 'namespace': 'example-namespace'}\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC, GRPCClientConfig\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "\n",
    "# Initialize a client.\n",
    "# API key is required, but the value does not matter.\n",
    "# Host and port of the Pinecone Local instance\n",
    "# is required when starting without indexes.\n",
    "pc = PineconeGRPC(api_key=\"pclocal\", host=\"http://localhost:5080\")\n",
    "\n",
    "# Create an index\n",
    "index_name = \"example-index\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=2,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "    time.sleep(1)\n",
    "\n",
    "# Target the index, disabling tls\n",
    "index_host = pc.describe_index(index_name).host\n",
    "index = pc.Index(host=index_host, grpc_config=GRPCClientConfig(secure=False))\n",
    "\n",
    "# Upsert records into the index\n",
    "index.upsert(\n",
    "    vectors=[\n",
    "        {\"id\": \"vec1\", \"values\": [1.0, -2.5], \"metadata\": {\"genre\": \"drama\"}},\n",
    "        {\"id\": \"vec2\", \"values\": [3.0, -2.0], \"metadata\": {\"genre\": \"documentary\"}},\n",
    "        {\"id\": \"vec3\", \"values\": [0.5, -1.5], \"metadata\": {\"genre\": \"documentary\"}},\n",
    "    ],\n",
    "    namespace=\"example-namespace\",\n",
    ")\n",
    "\n",
    "# Check the number of records in the index\n",
    "print(\"Index stats:\\n\", index.describe_index_stats())\n",
    "\n",
    "# Query the index with a metadata filter\n",
    "response = index.query(\n",
    "    vector=[3.0, -2.0],\n",
    "    filter={\"genre\": {\"$eq\": \"documentary\"}},\n",
    "    top_k=1,\n",
    "    include_values=True,\n",
    "    include_metadata=True,\n",
    "    namespace=\"example-namespace\",\n",
    ")\n",
    "\n",
    "print(\"\\nQuery response:\\n\", response)\n",
    "\n",
    "# Delete the index\n",
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9wSBV1-h6tH"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw7_rFAch6tH"
   },
   "source": [
    "We start by installing the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKjHHH10h6tH"
   },
   "outputs": [],
   "source": [
    "!pip install -qU semantic-router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5mvw1yUh6tK"
   },
   "source": [
    "We start by defining a dictionary mapping routes to example phrases that should trigger those routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nU02l4Mh6tK",
    "outputId": "f2b4dadd-14b0-4faa-af62-bdb6518b6bb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siraj\\Documents\\Personal\\Work\\Aurelio\\Virtual Environments\\semantic_router_3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from semantic_router import Route\n",
    "\n",
    "politics = Route(\n",
    "    name=\"politics\",\n",
    "    utterances=[\n",
    "        \"isn't politics the best thing ever\",\n",
    "        \"why don't you tell me about your political opinions\",\n",
    "        \"don't you just love the president\",\n",
    "        \"don't you just hate the president\",\n",
    "        \"they're going to destroy this country!\",\n",
    "        \"they will save the country!\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCSq2Ncbh6tL"
   },
   "source": [
    "Let's define another for good measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rkTc2ehh6tL"
   },
   "outputs": [],
   "source": [
    "chitchat = Route(\n",
    "    name=\"chitchat\",\n",
    "    utterances=[\n",
    "        \"how's the weather today?\",\n",
    "        \"how are things going?\",\n",
    "        \"lovely weather today\",\n",
    "        \"the weather is horrendous\",\n",
    "        \"let's go to the chippy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "routes = [politics, chitchat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWdFd8Neh6tM"
   },
   "source": [
    "Now we initialize our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiI5ob3Gh6tM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from semantic_router.encoders import CohereEncoder, OpenAIEncoder\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\") or getpass(\n",
    "    \"Enter Cohere API Key: \"\n",
    ")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "#     \"Enter OpenAI API Key: \"\n",
    "# )\n",
    "\n",
    "encoder = CohereEncoder()\n",
    "# encoder = OpenAIEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rebk9Z4yh6tM"
   },
   "source": [
    "Now we define the `RouteLayer`. When called, the route layer will consume text (a query) and output the category (`Route`) it belongs to — to initialize a `RouteLayer` we need our `encoder` model and a list of `routes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeP6ZUMoh6tM",
    "outputId": "6909f60c-41e2-4be5-eb25-8374e3d1461c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-07 16:02:43 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from semantic_router.layer import RouteLayer\n",
    "\n",
    "rl = RouteLayer(encoder=encoder, routes=routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF55Xqg1h6tN"
   },
   "source": [
    "Now we can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "126uJrBKh6tN",
    "outputId": "fca3e781-667e-42a9-e125-2c44cd314a09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name='politics', function_call=None, similarity_score=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"don't you love politics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRNUvo3dh6tN",
    "outputId": "c166e961-e897-48c4-b79f-fbbd678bd22e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name='chitchat', function_call=None, similarity_score=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"how's the weather today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQNc9OD4h6tN"
   },
   "source": [
    "Both are classified accurately, what if we send a query that is unrelated to our existing `Route` objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRKIToX3h6tO",
    "outputId": "b5795fb1-2045-4484-9858-377aeb98ba8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name=None, function_call=None, similarity_score=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"I'm interested in learning about llama 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVxcpE9Vh6tO"
   },
   "source": [
    "In this case, we return `None` because no matches were identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zJEY71mh6tO"
   },
   "source": [
    "# Demonstrating the Filter Feature\n",
    "\n",
    "Now, let's demonstrate the filter feature. We can specify a subset of routes to consider when making a classification. This can be useful if we want to restrict the scope of possible routes based on some context.\n",
    "\n",
    "For example, let's say we only want to consider the \"chitchat\" route for a particular query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8qIXBOWh6tO",
    "outputId": "37272c84-8189-477a-bdaa-7ce043fc203e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name='chitchat', function_call=None, similarity_score=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"don't you love politics?\", route_filter=[\"chitchat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQfEDKZgh6tO"
   },
   "source": [
    "Even though the query might be more related to the \"politics\" route, it will be classified as \"chitchat\" because we've restricted the routes to consider.\n",
    "\n",
    "Similarly, we can restrict it to the \"politics\" route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Owt-85Nh6tO",
    "outputId": "c5f9024f-7b70-4e3e-dcb1-3cb03688e8e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name=None, function_call=None, similarity_score=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"how's the weather today?\", route_filter=[\"politics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyhopRtUh6tP"
   },
   "source": [
    "In this case, it will return None because the query doesn't match the \"politics\" route well enough to pass the threshold.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
